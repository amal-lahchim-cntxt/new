{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfS/+U7wWtILAH4JAJMkdB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amal-lahchim-cntxt/new/blob/main/peaker_diarization_and_transcription_using_Whisper_and_Pyannote_within_Label_Studio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flWo8BfWB3fc"
      },
      "outputs": [],
      "source": [
        "!pip install openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "7r52JH4fB4eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyannote.audio"
      ],
      "metadata": {
        "id": "WglAJ0YIB4kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from label_studio_sdk import Client\n",
        "import requests\n",
        "from pyannote.audio import Pipeline\n",
        "from tqdm import tqdm\n",
        "import whisper\n",
        "\n",
        "# Load API key from environment variables or configuration file\n",
        "API_KEY = \"your_api_key_here\"\n",
        "LABEL_STUDIO_URL = \"https://your_label_studio_url_here\"\n",
        "\n",
        "client = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
        "\n",
        "try:\n",
        "    response = requests.get(f\"{LABEL_STUDIO_URL}/api/projects/\", headers={'Authorization': f'Token {API_KEY}'})\n",
        "    response.raise_for_status()\n",
        "    print(\"Connection successful. Projects data:\", response.json())\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(\"Failed to connect to Label Studio API:\", e)\n",
        "\n",
        "# Load diarization pipeline\n",
        "diarization_pipeline = Pipeline.from_pretrained(\n",
        "    \"pyannote/speaker-diarization\",\n",
        "    use_auth_token=\"your_huggingface_auth_token_here\"\n",
        ")\n",
        "\n",
        "# Load Whisper model\n",
        "whisper_model = whisper.load_model(\"large\")\n",
        "\n",
        "# Define project ID (replace with your actual project ID)\n",
        "project_id = 939\n",
        "project = client.get_project(project_id)\n",
        "tasks = project.get_tasks()\n",
        "\n",
        "for task in tqdm(tasks):\n",
        "    url = f'{LABEL_STUDIO_URL}{task[\"data\"][\"audio\"]}'\n",
        "    response = requests.get(url, headers={'Authorization': f'Token {API_KEY}'}, stream=True)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            audio_path = \"temp_audio.mp3\"\n",
        "            with open(audio_path, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "\n",
        "            # Perform diarization\n",
        "            diarization_result = diarization_pipeline({\"uri\": task[\"id\"], \"audio\": audio_path})\n",
        "            transcription_result = whisper_model.transcribe(audio_path)\n",
        "            segments = []\n",
        "\n",
        "            for segment in transcription_result['segments']:\n",
        "                start, end, text = segment['start'], segment['end'], segment['text']\n",
        "                speaker = 'Unknown'\n",
        "                for turn, _, spk in diarization_result.itertracks(yield_label=True):\n",
        "                    if turn.start <= start <= turn.end or turn.start <= end <= turn.end:\n",
        "                        speaker = spk\n",
        "                        break\n",
        "                segments.append({\n",
        "                    'from_name': 'speaker_transcription',\n",
        "                    'to_name': 'audio',\n",
        "                    'type': 'labels',\n",
        "                    'value': {\n",
        "                        'start': start,\n",
        "                        'end': end,\n",
        "                        'labels': [speaker],\n",
        "                        'text': [text]\n",
        "                    }\n",
        "                })\n",
        "            prediction = {\n",
        "                'result': segments,\n",
        "                'score': 1.0,\n",
        "                'model_version': \"whisper_diarization_combined\"\n",
        "            }\n",
        "            project.create_prediction(\n",
        "                task_id=task['id'],\n",
        "                result=prediction['result'],\n",
        "                score=prediction['score'],\n",
        "                model_version=prediction['model_version']\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio for task {task['id']}: {e}\")\n",
        "    else:\n",
        "        print(f\"Failed to fetch audio for task {task['id']}. Status code: {response.status_code}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "lwC6ahGoB4vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKYTp2u7XoOa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}